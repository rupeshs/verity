# ================================
# Main Configuration
# ================================

# OpenVINO device - CPU,GPU,NPU
DEVICE=CPU

# ================================
# LLM Configuration
# ================================
# LLM providers - openvino,ollama,openai
LLM_PROVIDER=openai
OLLAMA_HOST=http://host.docker.internal:11434
# OpenAI compatible chat endpoints
OPENAI_LLM_BASE_URL=http://host.docker.internal:9000
OPENAI_LLM_API_KEY=dummy

# For OpenVINO use rupeshs/jan-nano-int4-ov, works with CPU and GPU
# For NPU use rupeshs/jan-nano-int4-npu-ov
# For ollama use mannix/jan-nano:latest
# For openai LLM_MODEL_PATH=gpt-4o-mini
LLM_MODEL_PATH=rupeshs/jan-nano-int4-ov

# ================================
# Serach Configuration
# ================================
SEARXNG_BASE_URL=http://host.docker.internal:8080
NUM_SEARCH_RESULTS=3

# ================================
# API Server Configuration
# ================================
API_PORT=8000
API_HOST=0.0.0.0

